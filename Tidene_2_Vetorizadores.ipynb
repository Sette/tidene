{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidene Códigos - Vetorizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bruno/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/bruno/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "pd_corpus =  pd.read_csv('data/train_complete.csv',encoding='utf8')\n",
    "pd_corpus , corpus_test =  train_test_split(pd_corpus, train_size=0.7)\n",
    "classes_train = pd_corpus['sentiment']\n",
    "corpus = BeautifulSoup(str(pd_corpus['review']), \"html.parser\").get_text()\n",
    "corpus_test = corpus_test['review']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe que encapsula o tipo de tokenização e sequencia de limpeza a ser realizada nos textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilitarios - lê corpus e tokenizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "\n",
    "class readCorpus(object):\n",
    "    def __init__(self,csvfile,list_of_fields_to_read=[],tokenizer=None,encoding='utf8'):\n",
    "        self.csvfile = csvfile\n",
    "        self.fields = list_of_fields_to_read\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoding = encoding\n",
    "    \n",
    "    def __iter__(self):\n",
    "        f = open(self.csvfile,encoding=self.encoding, errors='ignore')\n",
    "        reader = csv.reader(f, delimiter=',', quoting=csv.QUOTE_MINIMAL) #separador dos campos\\n\",\n",
    "        headers = next(reader, None)\n",
    "        if (len(self.fields) <= 0):\n",
    "            self.fields = headers\n",
    "        selected_field_indexes = []\n",
    "        for idx,field in enumerate(headers):\n",
    "            if field in self.fields:\n",
    "                selected_field_indexes.append(idx)\n",
    "\n",
    "        for line in reader:\n",
    "            if line:\n",
    "                yield [line[idx] for idx in selected_field_indexes] if (len(selected_field_indexes)>1) else (line[selected_field_indexes[0]] if not self.tokenizer else tokenizer.tokenize(line[selected_field_indexes[0]]))\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import *    #https://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.tokenizer = nltk.tokenize.RegexpTokenizer(\"[a-zA-Z']+\")\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in self.tokenizer.tokenize(doc) if (len(t)>2)]\n",
    "\n",
    "class StemTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.tokenizer = nltk.tokenize.RegexpTokenizer(\"[a-zA-Z']+\")\n",
    "    def __call__(self, doc):\n",
    "        \n",
    "        return [self.stemmer.stem(t) for t in self.tokenizer.tokenize(doc) if (len(t)>2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/bruno/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define um vetorizador do tipo contagem de frequência (bag-of-words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=stop_words) # usa a classe de tokenizacao definida acima\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passa os textos pelo vetorizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = readCorpus(\"toy.csv\",list_of_fields_to_read=['data'])\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(corpus) # resulta em uma matriz sparsa numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_counts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O vetorizador também pode ser utilizado para transformar um texto não visto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_nao_visto = count_vectorizer.transform(['New film review'])\n",
    "print(texto_nao_visto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A partir daí temos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Formato Matriz sparsa gerada (numdocs,features) ==> \", X_train_counts.shape)\n",
    "print(\"Representacao de um documento (o 6º) ==> \", X_train_counts[5])\n",
    "print(\"Indice de uma palavra ('film') ==>\", count_vectorizer.vocabulary_.get('film'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explorando as contagens (palavras mais frequentes no texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\n",
    "sum_words = X_train_counts.sum(axis=0)    #bag_of_words\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in count_vectorizer.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define um vetorizador do tipo contagem de frequência de bi-gramas (na tentativa de juntar palavras que aparecem sempre juntas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2), tokenizer=LemmaTokenizer(), stop_words='english', min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = readCorpus(\"toy.csv\",list_of_fields_to_read=['data'])\n",
    "X_train_bigram_counts = bigram_vectorizer.fit_transform(corpus) # resulta em uma matriz sparsa numpy\n",
    "#print('Dicionario: %s' %bigram_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explorando as contagens (palavras mais frequentes no texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\n",
    "sum_bigrams = X_train_bigram_counts.sum(axis=0)    #bag_of_words\n",
    "bigram_freq = [(bigram, sum_bigrams[0, idx]) for bigram, idx in bigram_vectorizer.vocabulary_.items()]\n",
    "bigram_freq =sorted(bigram_freq, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define um vetorizador do tipo TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words='english', min_df=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cria um vetorizador já ajustado ao texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruno/anaconda3/envs/curso/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "#corpus = readCorpus(\"toy.csv\",list_of_fields_to_read=['data'])\n",
    "classes = pd_corpus['sentiment'].values.tolist()\n",
    "\n",
    "## NOS OUTROS EXEMPLOS ESSAS DUAS ACOES (FIT E TRANSFORM) SAO FEITAS EM UMA SO, ATRAVES DO FIT_TRANSFORM ..\n",
    "##   FIZ A SEPARACAO AQUI PARA PODER GRAVAR NO DISCO O VETORIZADOR E PODER USA-LO PARA VETORIZAR TEXTOS NOVOS DEPOIS\n",
    "tfidf_vectorizer = tfidf_vectorizer.fit(corpus,classes) # treina o vetorizador\n",
    "X_train_tfidf = tfidf_vectorizer.transform(corpus) # transforma os textos em uma matriz sparsa numpy\n",
    "\n",
    "# gravando no disco vetorizador e a matriz vetorizada\n",
    "pickle.dump(tfidf_vectorizer, open(\"tfidf_vectorizer.pickle\", \"wb\"))\n",
    "pickle.dump(X_train_tfidf, open(\"X_train_tfidf.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Formato Matriz sparsa gerada (numdocs,features) ==> \", X_train_tfidf.shape)\n",
    "print(\"Indice de uma palavra ==>\", tfidf_vectorizer.vocabulary_.get('film'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Representacao de um documento ==> \", X_train_tfidf[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dicionario: %s' %tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explorando os dados (ordenando as palavras com maior tfidf nos documentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\n",
    "sum_idfs = X_train_tfidf.sum(axis=0)    #tfidfs\n",
    "words_idfs = [(word, sum_idfs[0, idx]) for word, idx in tfidf_vectorizer.vocabulary_.items()]\n",
    "words_idfs =sorted(words_idfs, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Montando vetorizador tfidf para bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_bigram_vectorizer = TfidfVectorizer(ngram_range=(2, 2), tokenizer=LemmaTokenizer(), stop_words='english') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = readCorpus(\"toy.csv\",list_of_fields_to_read=['data'])\n",
    "X_train_bigram_tfidf = tfidf_bigram_vectorizer.fit_transform(corpus) # resulta em uma matriz sparsa numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\n",
    "sum_bigrams_tfidf = X_train_bigram_tfidf.sum(axis=0)    #bag_of_words\n",
    "bigram_tfidf_freq = [(bigram, sum_bigrams_tfidf[0, idx]) for bigram, idx in tfidf_bigram_vectorizer.vocabulary_.items()]\n",
    "bigram_tfidf_freq =sorted(bigram_tfidf_freq, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_tfidf_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vetorização com o word2vec do Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# utilizando um modelo word2vec previamente construido\n",
    "import gensim.downloader as api\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://radimrehurek.com/gensim/models/word2vec.html\n",
    "# representacao de uma palavra no formato word2vec = representado na dimensao 100 do modelo utilizado\n",
    "print(word_vectors.get_vector('film'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NAO UTILIZAR COM TOKENS STEMMIZADOS = PQ ELE NAO ACHARA MUITAS PALAVRAS\n",
    "import numpy as np\n",
    "class Word2VecVectorizer:\n",
    "  def __init__(self,word_vectors):\n",
    "    self.word_vectors = word_vectors\n",
    "\n",
    "  def fit(self, lst_tokens):\n",
    "    pass\n",
    "\n",
    "  # para cada sentenca tokenizada ele representa cada palavra segundo a \n",
    "    #representacao w2v e depois tira a media de todas as palavras da sentenca\n",
    "  def transform(self, lst_tokens): #pega uma lista de tokens\n",
    "    self.D = word_vectors.get_vector(word_vectors.index2word[0]).shape[0]\n",
    "    X = np.zeros((len(lst_tokens), self.D))\n",
    "    n = 0\n",
    "    emptycount = 0\n",
    "    for tokens in lst_tokens:\n",
    "      vecs = []\n",
    "      m = 0\n",
    "      for word in tokens:\n",
    "        try:\n",
    "            vec = self.word_vectors.get_vector(word)\n",
    "            vecs.append(vec)\n",
    "            m += 1\n",
    "        except KeyError:\n",
    "            #print('Palavra ',word,' nao pode ser representada')\n",
    "            pass\n",
    "      if len(vecs) > 0:\n",
    "        vecs = np.array(vecs)\n",
    "        X[n] = vecs.mean(axis=0)\n",
    "      else:\n",
    "        emptycount += 1\n",
    "      n += 1\n",
    "    return X\n",
    "\n",
    "\n",
    "  def fit_transform(self, lst_tokens):\n",
    "    self.fit(lst_tokens)\n",
    "    return self.transform(lst_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizando vetorizar os textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# usa o dictionario e o bowcorpus para recuperar os tokens\n",
    "# lembrando que o bowcorpus nao devera ser construido com stems ..ele pode nao conseguir representar muitas palavras\n",
    "dictionary = gensim.corpora.Dictionary.load(\"dictionary.dict\") #carrega o dicionario do disco\n",
    "idx2wordDictionary = {k:v for k, v in dictionary.iteritems()} #mudando a ordem dos indices\n",
    "lem = LemmaTokenizer()\n",
    "bowcorpus = gensim.corpora.MmCorpus('bowcorpus.mm') # le o corpus representado em bag-of-words\n",
    "tokens = [[idx2wordDictionary[idx] for idx,freq in text] for text in bowcorpus]\n",
    "print(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vectorizer_glove_wiki_gigaword_100 = Word2VecVectorizer(word_vectors)\n",
    "\n",
    "X_train_w2v_glove_wiki_gigaword_100 = w2v_vectorizer_glove_wiki_gigaword_100.fit_transform(tokens)\n",
    "\n",
    "#np.save('w2vmatrix.npy', textos_vetorizados) # salvando no disco\n",
    "\n",
    "# gravando no disco vetorizador e a matriz vetorizada\n",
    "pickle.dump(X_train_w2v_glove_wiki_gigaword_100, open(\"X_train_w2v_glove_wiki_gigaword_100.pickle\", \"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_w2v_glove_wiki_gigaword_100[0]) #textos_vetorizados[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v_glove_wiki_gigaword_100.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformando textos de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_test = readCorpus(\"testtoy.csv\",list_of_fields_to_read=['data'])\n",
    "\n",
    "lem = LemmaTokenizer()\n",
    "\n",
    "test_lem_tokens = [lem(texto) for texto in corpus_test]\n",
    "print(test_lem_tokens[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_w2v_glove_wiki_gigaword_100 = w2v_vectorizer_glove_wiki_gigaword_100.fit_transform(test_lem_tokens)\n",
    "\n",
    "# gravando no disco vetorizador e a matriz vetorizada\n",
    "pickle.dump(X_test_w2v_glove_wiki_gigaword_100, open(\"X_test_w2v_glove_wiki_gigaword_100.pickle\", \"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
