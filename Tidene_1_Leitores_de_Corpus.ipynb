{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Códigos Tidene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura dos textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opção 1: Leitura de corpus (textos) de tamanho muito grande\n",
    "\n",
    "Classe readCorpus - Permite a extração de colunas específicas.\n",
    "Requisitos: O arquivo csv deve ter uma linha de cabeçalho, que nomeia cada um dos colunas (campos)\n",
    "\n",
    "Parâmetros de entrada:\n",
    "   - csvfile => nome do arquivo csv\n",
    "   - list_of_fields_to_read=[] ==> lista de colunas que deverão ser lidas (se não colocar valor, ele assume que deverá ler os valores de todas as colunas)\n",
    "   - tokenizer = None => recebe um objeto do tipo tokenizador (caso tenha valor, retornará o texto já tokenizado utilizando aquele tokenizador) == vale apenas para lista de campos = 1\n",
    "   - encoding => padrão de codificação (default = utf8)\n",
    "\n",
    "Saída: iterador que percorre cada linha do corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo de entrada .csv\n",
    "\n",
    "#### subgroup,maingroup,subclas,clas,section,othersipcs,data\n",
    "B03B00402,B03B004,B03B,B03,B,B07B00408,separation apparatus this invention relates to a method for separation of a light material from a heavier material a separation table of vibrator type and a cyclone and a\n",
    "\n",
    "B03B00500,B03B005,B03B,B03,B,B01D01102-E02Fn means00388,method and installation for desalinating sand and suction hopper comprising such an installation the invention \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "class readCorpus(object):\n",
    "    def __init__(self,csvfile,list_of_fields_to_read=[],tokenizer=None,encoding='utf8'):\n",
    "        self.csvfile = csvfile\n",
    "        self.fields = list_of_fields_to_read\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoding = encoding\n",
    "    \n",
    "    def __iter__(self):\n",
    "        f = open(self.csvfile,encoding=self.encoding, errors='ignore')\n",
    "        reader = csv.reader(f, delimiter=',', quoting=csv.QUOTE_MINIMAL) #separador dos campos\\n\",\n",
    "        headers = next(reader, None)\n",
    "        if (len(self.fields) <= 0):\n",
    "            self.fields = headers\n",
    "        selected_field_indexes = []\n",
    "        for idx,field in enumerate(headers):\n",
    "            if field in self.fields:\n",
    "                selected_field_indexes.append(idx)\n",
    "\n",
    "        for line in reader:\n",
    "            if line:\n",
    "                yield [line[idx] for idx in selected_field_indexes] if (len(selected_field_indexes)>1) else (line[selected_field_indexes[0]] if not self.tokenizer else tokenizer.tokenize(line[selected_field_indexes[0]]))\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo de uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = readCorpus(\"data/train_min.csv\",list_of_fields_to_read=['sentiment','review'])\n",
    "textos = [texto for texto in corpus]\n",
    "print(textos[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se quiser armazenar em uma estrutura do tipo DataFrame\n",
    "import pandas as pd\n",
    "df_textos = pd.DataFrame(textos,columns=['sentiment','review']) # armazenando somente os textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opção 2: Ler direto do arquivo .csv em uma estrutura tipo DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_textos = pd.read_csv('data/train.csv',encoding='utf8')['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_textos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpeza dos textos + redução de dimensionalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import *\n",
    "\n",
    "# instancia o tokenizador\n",
    "tokenizer=nltk.tokenize.RegexpTokenizer(\"[a-zA-Z']+\")\n",
    "\n",
    "# ... este, por exemplo, separa por palvras e deixa as que tem ' juntas \n",
    "# exemplo de uso\n",
    "tokenizer.tokenize(\"my can't go should't 321\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = readCorpus(\"data/train_min.csv\",list_of_fields_to_read=['review'],tokenizer=tokenizer)\n",
    "tokens = [texto for texto in corpus]   #values.tolist()\n",
    "print(tokens[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lista de stopwords disponível na nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_noStp = [[word for word in texto if not word in stop_words and len(word) > 1] for texto in tokens]\n",
    "print(tokens_noStp[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remoção de radicais (utilizando lemmatizador ou stemmer)\n",
    "\n",
    "(https://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lematizador\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tokens_lem = [[wordnet_lemmatizer.lemmatize(word) for word in texto] for texto in tokens_noStp]\n",
    "print(tokens_lem[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "tokens_stem = [[porter_stemmer.stem(word) for word in texto] for texto in tokens_noStp]\n",
    "print(tokens_stem[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando no disco o corpus serializado em forma de (indice,frequencia)\n",
    "\n",
    "$conda install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primeiro, monta-se o dicionario (em forma de indice, palavra unica)\n",
    "dictionary = gensim.corpora.Dictionary(tokens_lem)\n",
    "dictionary.save('dictionary.dict')\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representação Bag-of-Words (contagem de palavras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load(\"dictionary.dict\") #carrega o dicionario do disco\n",
    "\n",
    "bowcorpus = [dictionary.doc2bow(texto) for texto in tokens_lem] #vetoriza para representacao (indice,freq)\n",
    "gensim.corpora.MmCorpus.serialize('bowcorpus.mm', bowcorpus)  # grava no disco\n",
    "print(bowcorpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representação Tf-idf (https://radimrehurek.com/gensim/tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowcorpus = gensim.corpora.MmCorpus('bowcorpus.mm')\n",
    "tfidf_vectorizer = gensim.models.TfidfModel(bowcorpus)\n",
    "tfidf_corpus_matrix = tfidf_vectorizer[bowcorpus]\n",
    "gensim.corpora.MmCorpus.serialize('tfidf_corpus_matrix.mm', tfidf_corpus_matrix)  # grava no disco\n",
    "\n",
    "print(tfidf_corpus_matrix[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
