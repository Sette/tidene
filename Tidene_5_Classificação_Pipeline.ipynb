{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score,accuracy_score\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopwords = stopwords.words('english')\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        if len(word2vec) > 0:\n",
    "            self.dim = len(word2vec)\n",
    "        else:\n",
    "            self.dim = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "])\n",
    "\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        if len(word2vec) > 0:\n",
    "            self.dim = len(word2vec)\n",
    "        else:\n",
    "            self.dim = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer= lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of\n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                     for w in words if w in self.word2vec] or\n",
    "                    [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset_Review(file,clas):\n",
    "    X, y = [], []\n",
    "    for index,line in file.iterrows():\n",
    "        review_text = BeautifulSoup(line[\"review\"], \"html.parser\").get_text()\n",
    "        #\n",
    "        # 2. Remove caractéres não alfa-numéricos\n",
    "        review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text).lower()\n",
    "        review_text = [w for w in tokenizer.tokenize(review_text) if w not in stopwords]\n",
    "        if clas == True:\n",
    "            label = line['sentiment']\n",
    "            y.append(label)\n",
    "        text = review_text\n",
    "        X.append((text))\n",
    "        \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = pd.read_csv('labeledTrainData.tsv',delimiter=\"\\t\", quoting=3)\n",
    "train_unl = pd.read_csv('unlabeledTrainData.tsv',delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "X_w2v, y_w2v = loadDataset_Review(train,True)\n",
    "\n",
    "X_w2v_unl, y_w2v_unl = loadDataset_Review(train_unl,False)\n",
    "\n",
    "X_w2v = np.concatenate((X_w2v, X_w2v_unl), axis=0)\n",
    "\n",
    "train, test = train_test_split(train, test_size=0.3, train_size=0.7, random_state=42)\n",
    "\n",
    "X,y = loadDataset_Review(train, True)\n",
    "\n",
    "X_test,y_test = loadDataset_Review(test, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura valores para o word2vec\n",
    "num_features = 300  # Word vector dimensionality\n",
    "min_word_count = 40  # Minimum word count\n",
    "num_workers = 4  # Number of threads to run in parallel\n",
    "context = 10  # Context window size\n",
    "downsampling = 1e-3  # Downsample setting for frequent words\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "try:\n",
    "    model = gensim.models.Word2Vec.load(model_name)\n",
    "except:\n",
    "    model = gensim.models.Word2Vec(X_w2v, workers=num_workers, \\\n",
    "                size=num_features, min_count=min_word_count, \\\n",
    "                window=context, sample=downsampling, seed=1)\n",
    "    model.save(model_name)\n",
    "    print(\"Gerou o modelo\")\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mult_nb = Pipeline(\n",
    "    [(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "\n",
    "mult_nb_tfidf = Pipeline(\n",
    "    [(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
    "\n",
    "bern_nb_tfidf = Pipeline(\n",
    "    [(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "\n",
    "svc = Pipeline(\n",
    "    [(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"linear svc\", LinearSVC(max_iter=10**4))])\n",
    "\n",
    "svc_tfidf = Pipeline(\n",
    "    [(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"linear svc\", LinearSVC(max_iter=10**4))])\n",
    "\n",
    "etree_w2v = Pipeline(\n",
    "    [(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),(\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "\n",
    "etree_w2v_tfidf = Pipeline(\n",
    "    [(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)),\n",
    "                                (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "\n",
    "random_w2v_tfidf = Pipeline(\n",
    "    [(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)),\n",
    "                                (\"extra trees\", RandomForestClassifier(n_estimators=200))])\n",
    "random_w2v = Pipeline(\n",
    "    [(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "(\"extra trees\", RandomForestClassifier(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = [\n",
    "        (\"mult_nb\", mult_nb),\n",
    "        (\"mult_nb_tfidf\", mult_nb_tfidf),\n",
    "        (\"bern_nb\", bern_nb),\n",
    "        (\"bern_nb_tfidf\", bern_nb_tfidf),\n",
    "        (\"Linearsvc\", svc),\n",
    "        (\"Linearsvc_tfidf\", svc_tfidf),\n",
    "        (\"w2v\", etree_w2v),\n",
    "        (\"w2v_tfidf\", etree_w2v_tfidf),\n",
    "        (\"random_w2v\", random_w2v),\n",
    "        (\"random_w2v_tfidf\", random_w2v_tfidf),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with  mult_nb\n",
      "Training with  mult_nb_tfidf\n",
      "Training with  bern_nb\n",
      "Training with  bern_nb_tfidf\n",
      "Training with  Linearsvc\n",
      "Training with  Linearsvc_tfidf\n",
      "Training with  w2v\n",
      "Passou\n",
      "Passou\n",
      "Training with  w2v_tfidf\n",
      "Training with  random_w2v\n",
      "Passou\n",
      "Passou\n",
      "Training with  random_w2v_tfidf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unsorted_scores = []\n",
    "for name, model in all_models:\n",
    "    print(\"Training with \", name)\n",
    "    predict = model.fit(X,y).predict(X_test)\n",
    "    unsorted_scores.append((name,accuracy_score(y_test,predict),\\\n",
    "    f1_score(y_test,predict), precision_score(y_test,predict),recall_score(y_test,predict)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model               Accuracy      F1    Precision    Recall\n",
      "----------------  ----------  ------  -----------  --------\n",
      "Linearsvc_tfidf       0.8912  0.8923       0.8860    0.8987\n",
      "mult_nb_tfidf         0.8677  0.8671       0.8739    0.8604\n",
      "Linearsvc             0.8641  0.8649       0.8627    0.8671\n",
      "mult_nb               0.8627  0.8610       0.8745    0.8480\n",
      "random_w2v            0.8567  0.8606       0.8404    0.8817\n",
      "random_w2v_tfidf      0.8544  0.8582       0.8390    0.8783\n",
      "w2v                   0.8539  0.8574       0.8397    0.8759\n",
      "w2v_tfidf             0.8524  0.8562       0.8374    0.8759\n",
      "bern_nb               0.8485  0.8426       0.8799    0.8083\n",
      "bern_nb_tfidf         0.8485  0.8426       0.8799    0.8083\n"
     ]
    }
   ],
   "source": [
    "scores = sorted(unsorted_scores, key=lambda x: -x[1])\n",
    "\n",
    "print(tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'Accuracy','F1','Precision','Recall')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
